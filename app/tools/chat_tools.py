from langchain_core.tools import tool
from app.services import chat_service


@tool("chat_over_file")
def chat_over_file(file: str, query: str) -> str:
    """
    Answer a natural language question grounded in the given uploaded file.

    Args:
        file: The filename under the uploads directory that has been ingested.
        query: The user's question to answer using the file's content.

    Returns:
        A string answer generated by the model and retrieval pipeline.
    """
    result = chat_service.answer(file, query)
    # chat_service.answer returns a dict with key "response"
    if isinstance(result, dict) and "response" in result:
        return str(result["response"])  # type: ignore[index]
    return str(result)

