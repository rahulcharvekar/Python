from langchain_core.tools import tool
from app.services import chat_service


@tool("chat_over_file")
def chat_over_file(file: str, query: str) -> str:
    """
    Answer a natural language question grounded in the given uploaded file.

    Args:
        file: The filename under the uploads directory that has been ingested.
        query: The user's question to answer using the file's content.

    Returns:
        A string answer generated by the model and retrieval pipeline.
    """
    result = chat_service.answer(file, query)
    # chat_service.answer returns a dict with key "response"
    if isinstance(result, dict) and "response" in result:
        return str(result["response"])  # type: ignore[index]
    return str(result)


@tool("chat_over_profile")
def chat_over_profile(file: str, query: str) -> str:
    """
    Answer a question grounded in the profile file with memory support
    and a slightly lower retrieval threshold for better recall.
    """
    # Profile-specific: lower threshold, strict retrieval (no memory)
    result = chat_service.answer(
        file,
        query,
        k=12,
        score_threshold=0.45,
        strict=True,
    )
    if isinstance(result, dict) and "response" in result:
        return str(result["response"])  # type: ignore[index]
    return str(result)
